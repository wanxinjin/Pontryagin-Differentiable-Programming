<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework">
  <meta name="keywords" content="Differentiable Programming, Learning, Conrol, Robot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>





<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Pontryagin Differentiable Programming:</h1>
          <h1 class="title is-2 publication-title">An End-to-End Learning and Control Framework </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wanxinjin.github.io/" target="_blank">Wanxin Jin </a><sup>1</sup>,&nbsp&nbsp</span>

            <span class="author-block">
              <a href="https://zhaoranwang.github.io/" target="_blank">Zhaoran Wang</a><sup>2&nbsp</sup>,&nbsp&nbsp</span>


            <span class="author-block">
              <a href="https://zhuoranyang.github.io/" target="_blank">Zhuoran Yang</a><sup>3</sup>,&nbsp&nbsp</span>

            <span class="author-block">
              <a href="https://engineering.purdue.edu/AAE/people/ptProfile?resource_id=124981" target="_blank">Shaoshuai Mou</a><sup>4</sup>,&nbsp&nbsp</span>
            </span>

            <span class="author-block">
              <a href="https://www.georgejpappas.org/" target="_blank">George J. Pappas</a><sup>5</sup>
            </span>


          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Arizona State University,&nbsp&nbsp</span>
            <span class="author-block"><sup>2</sup>Northwestern University,&nbsp&nbsp</span>
            <span class="author-block"><sup>3</sup>Yale University,&nbsp&nbsp</span><br>
            <span class="author-block"><sup>4</sup>Purdue University,&nbsp&nbsp</span>
            <span class="author-block"><sup>5</sup>University of Pennsylvania</span>
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://proceedings.neurips.cc/paper/2020/hash/5a7b238ba0f6502e5d6be14424b20ded-Abstract.html" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>NeurIPS Paper 1</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://papers.nips.cc/paper/2021/hash/85ea6fd7a2ca3960d0cf5201933ac998-Abstract.html" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>NeurIPS Paper 2</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/wanxinjin/Pontryagin-Differentiable-Programming" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code Repo 1</span>
                </a>
              </span>
                            <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/wanxinjin/Safe-PDP" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code Repo 2</span>
                </a>
              </span>

              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/askforalfred/alfred"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <b>Overview:</b> We devleoped a unified differentiable framework that is able to efficiently learn/auto-tune different components of a general optimal control system: (neural) cost/reward, (neural) dynamics ODE, (neural) policy, (neural) constraints, (neural)  trajectory, (neural) initial conditions.
       <br><br><br>
                        <figure>
        <img src="./static/images/pdp-overview.png" width="80%" class="center"/>
        <br><br>
    <figcaption style="text-align: center; color: #888;">
      The overview of Pontryagin Differentiable Programming framework
      </figcaption>
    </figure>
      </h2>



      
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered text-align: center">
      <div class="column is-four-fifths">
        <div class="columns is-centered text-align: center">
        <h2 class="title is-3">Abstract</h2>
      </div>
        <div class="content text-align: left">
          <p>We developed the Pontryagin Differentiable Programming (PDP) methodology, which provides a unified differentiable solution to solve a broad class of learning and control tasks that can be formulated with a general optimal control system. The PDP features two new techniques: first, we proposed Differential Pontryagin's Maximum Principle, through which we can obtain the analytical derivative of a trajectory with respect to the tunable parameters in any components of the optimal control system, enabling auto-tuning dynamics ODE, policy, constraints, initial conditions, or/and cost/reward function; and second, we proposed the  "Auxiliary Control System" in the backpropagation of the differentation path, and the trajectory of the Auxiliary Control System is  exactly the analytical derivative of the original system's trajectory with respect to the parameters. This   "Auxiliary Control System" makes the gradient computation through a  optiaml control system more computationally efficient.
          </p>
          <p>
            The PDP  enables learning/auto-tuning different components in a general optimal control system: (neural) cost/reward, (neural)  dynamics ODE, (neural) policy, (neural) constraints, (neural)  trajectory, (neural) initial conditions.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        

        <h2 class="title is-3">1. PDP Formulation (abstract)</h2>


<body>
  
<p>
Consider a class of optimal control systems \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\), which are parameterized by a tunable parameter \(\textcolor{red}{\boldsymbol{\theta}}\) in its  cost, dynamics ODE, initial condition, and constraints:
<br> <br>

  <div class="columns is-centered" align="center">
    <figure>
        <img src="./static/images/ocs.png" width="95%"/>
    </figure>
  </div>

</p>


<p>
We here have parameterized all components in a general optimal control system \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\), for a specific application, one  only needs to parameterize the unknown aspects  and keep others given. Any  unknown components in \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\) can be implemented by differentiable neural networks. 




<br><br>



For  a given  \(\textcolor{red}{\boldsymbol{\theta}}\),  \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\) produces a  trajectory \(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}}\).
We aim to find/tune/learn a \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}^*})\), i.e, searching for a \(\textcolor{red}{\boldsymbol{\theta}^*}\), such that its    trajectory  \(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\boldsymbol{\theta}^*}}}\)
minimizes a design criterion \(L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})\). Note that \(L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})\) is given depending on the specific design task. The  PDP problem can be formally written as:
\begin{equation}\label{equ_problem}
\begin{aligned}
\min_{\textcolor{red}{\boldsymbol{\theta}}} \quad  &L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}}) \quad \\  \text{subject to}\quad &
 \boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}} \,\, \text{comes from  $\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})$.}
\end{aligned}\tag*{PDP Problem}
\end{equation}
For a specific learning/control task, one only needs to specify   details of \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\)
and give a   loss  \(L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})\).
</p>
</body>



<br><br><br>

 <h2 class="title is-3">2. PDP Method</h2>
<body>
  
  <p>


To solve the  \ref{equ_problem}, we seeks to optimize the design loss \(L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})\) using any gradient-based method, with the following update


  \begin{equation}\label{GD}
  \textcolor{red}{\boldsymbol{{\theta}}^+}=\textcolor{red}{\boldsymbol{{\theta}}}-\eta\frac{d L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})}{d \textcolor{red}{\boldsymbol{\theta}}} \quad \text{with (chain rule):}\quad \frac{d L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})}{d \textcolor{red}{\boldsymbol{\theta}}}=\frac{d L}{d   \boldsymbol{{\xi}}_{\textcolor{red}{\boldsymbol{{\theta}}}}}\frac{d  \boldsymbol{{\xi}}_{\textcolor{red}{\boldsymbol{{\theta}}}}}{d \textcolor{red}{\boldsymbol{{\theta}}}}
  \end{equation}

 
Thus, in each update of \(\textcolor{red}{\boldsymbol{{\theta}}}\), the  comptutation framework is drawn below 

<br><br>


  <div class="columns is-centered" align="center">
    <figure>
        <img src="./static/images/gradient.png" width="70%"/>
    </figure>
  </div>


<p>
  The main challenge is to how to do  auto-differentiation to calculate \(\frac{d  \boldsymbol{{\xi}}_{\textcolor{red}{\boldsymbol{{\theta}}}}}{d \textcolor{red}{\boldsymbol{{\theta}}}}\), i.e., the derivative of a trajectory with respect to the parameters in the general optimal control system. 

<br><br>


The key contributions of PDP are that we proposed the <strong>Differential Pontryagin's Maximum Principle (Differential PMP)</strong>, based on which we invented the <strong>Auxiliary Control System</strong>, as  a systematic tool to do the auto-differentiation. Specifically, <strong>Auxiliary Control System</strong> takes the original system's trajectory \(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}}\) and efficiently generates \(\frac{d  \boldsymbol{{\xi}}_{\textcolor{red}{\boldsymbol{{\theta}}}}}{d \textcolor{red}{\boldsymbol{{\theta}}}}\). Below is the illustration to show how it looks like and works.

</p>


<br><br>

  <div class="columns is-centered" align="center">
    <figure>
        <img src="./static/images/acs.png" width="80%"/>
    </figure>
  </div>

<br><br>


<p>
<strong>Takeaways 1: [Dual Structure]</strong>
The Auxiliary Control System is very easy to construct and possesses a dual structure  for the original  system. If the original system is optimal control system, then the  Auxiliary Control System is a Linear Quadratic Regular (LQR); if the original system is feedback control system, then the corresponding Auxiliary Control System is also a feedback control system; if the original system is ODE (i.e. only including dynanmics), then the corresponding Auxiliary Control System is also a ODE. 
<br>

<br>


<strong>Takeaways 2: [Existence and Unique Solution]</strong>
If the original system is differentiable (see the differentiablity condition in our papers), then Auxiliary Control System must exist and have a unique global solution.
<br>
<br>



<strong>Takeaways 3: [Working perfectly with Interior Point Methods]</strong>
The  PDP including the Auxiliary Control System works seamlessly with <a href="https://www.sciencedirect.com/science/article/pii/S0377042700004337" target="_blank">Interior Point Methods</a>  [Potra, Florian A.; Stephen J. Wright. "Interior-point methods"]. Under the context of Interior-point method, we have  proved that "the differentiablity of the relaxed optimal control system is guaranteed and continuous", i.e., 

\begin{equation}
\frac{d  \boldsymbol{{\xi}}_{\textcolor{red}{\boldsymbol{{\theta}}}}(\textcolor{green}{\gamma})}{d \textcolor{red}{\boldsymbol{{\theta}}}}\rightarrow \frac{d  \boldsymbol{{\xi}}_{\textcolor{red}{\boldsymbol{{\theta}}}}}{d \textcolor{red}{\boldsymbol{{\theta}}}}\quad \text{as} \quad \textcolor{green}{\gamma}\rightarrow 0
\end{equation}
where \(\textcolor{green}{\gamma}\) is the barrier penality parameter in the relaxed optimal control system (see more formal assertions in our papers)


<br>



  </p>
</body>
 


<br>
<br>
<br>
<br>


 <h2 class="title is-3">3. Applications of PDP</h2>


<p>
    Depending upon the details of \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\), i.e., what is unknown (thus parameterized), and the specific  loss \(L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})\), PDP can be used to solve various design/learning/planning problems. Here are a few simple examples.
</p>

<br><br>


  
 <h2 class="title is-4"> 3.1. &nbsp Inverse Reinforcement Learning </h2>
<p>
    Suppose we want to learn  reward/cost  from expert (optimal) demostrations. Thus, we parameterize the cost  in \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\) and define \(L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})\) to be the imitation loss. Then, the \ref{equ_problem} is a inverse reinforcement learning problem. Here are some demo results.
<br>
<iframe width="440" height="315" src="https://www.youtube.com/embed/awVNiCIJCfs?si=n6xJ1YzZb-V-iI0o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe width="440" height="315" src="https://www.youtube.com/embed/4RxDLxUcMp4?si=1T_6A_IMaTW_GoDq" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


</p>

<br><br>

 <h2 class="title is-4"> 3.2. &nbsp System Identification (Neural ODE) </h2>
<p>
    Suppose that we want to learn or identify dynamics ODE from meansured input-state data of a black-box system. Thus, we parameterize the dynamics ODE in \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\) and define \(L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})\) to be the supervised learning loss. Then, the \ref{equ_problem} is SysID (Neural ODE) problem. Here is one demo result.


    <iframe width="440" height="315" src="https://www.youtube.com/embed/PAyBZjDD6OY?si=f2aRnd87DfMdExVm" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</p>

<br><br>

 <h2 class="title is-4"> 3.3. &nbsp  Policy Optimization or Motion Planning (model-based) </h2>
<p>
    Suppose that we want to find  optimal policy (closed-loop) or plan (open loop) for a robotic system (assume the system dynamics has been identified above). Thus, in \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\) we parameterize the policy \(u_t=\pi(x_t,\textcolor{red}{\boldsymbol{\theta}})\) for policy optimization or trajectory \(u_t=\pi(t, \textcolor{red}{\boldsymbol{\theta}})\) for motion planning,  and define \(L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})\) to be  reward or cost function. Then, the \ref{equ_problem} is to solve the policy optimization or motion planning problem. Here are some demo results.
</p>

<iframe width="440" height="315" src="https://www.youtube.com/embed/sC81qc2ip8U?si=kZZDbC5AOc0T2teV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe width="440" height="315" src="https://www.youtube.com/embed/5Jsu772Sqcg?si=jsgf6x2Psbf3ZgDW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>



<br><br><br>

 <h2 class="title is-4"> 3.4. &nbsp  Learning Constraints  from Demonstrations </h2>
<p>
    Suppose we want to design a MPC controller, where we don't know how to design the constraints. We can learn those constraints from the expert demonstration data. In particular, we parameterize the constraints in \(\textbf{OCS}(\textcolor{red}{\boldsymbol{\theta}})\) and define \(L(\boldsymbol{\xi}_{\textcolor{red}{\boldsymbol{\theta}}})\) to be the imitation loss. Then, the \ref{equ_problem} is a problem of learning constraints from demonstrations. Here is one demo result.

    <iframe width="440" height="315" src="https://www.youtube.com/embed/OBiLYYlWi98?si=yK8GVI2mzvGvbLQt" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</p>

<br><br>

 <h2 class="title is-4"> 3.5. &nbsp  Learning from Sparse Demonstrations </h2>
<p>
    As a novel application, in our  <a href="https://ieeexplore.ieee.org/document/9849419?denied=" target="_blank">T-RO paper</a>, we have demonstrated how to use PDP to solve the problelm of learning from sparse demonstration. 


</p>


    <iframe width="560" height="315" src="https://www.youtube.com/embed/BYAsqMxW5Z4?si=FM48KHf1CgSyD5Ct" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>



<br><br><br><br><br>

 <h2 class="title is-3">4. Research Papers that directly use PDP</h2>
<body>
  
  <p>




[1] Kun Cao, and Lihua Xie. "Trust-Region Inverse Reinforcement Learning." IEEE Transactions on Automatic Control, 2023.
<br><br>

[2] Ming Xu, Timothy L. Molloy, and Stephen Gould. "Revisiting Implicit Differentiation for Learning Problems in Optimal Control." Advances in Neural Information Processing Systems, 2023.
<br><br>

[3] Wanxin Jin, Todd Murphey, Dana Kulic, Neta Ezer, and Shaoshuai Mou. "Learning from sparse demonstrations." IEEE Transactions on Robotics, 2022.
<br><br>

[4] Kun Cao, and Lihua Xie. Game-Theoretic Inverse Reinforcement Learning: A Differential Pontryagin's Maximum Principle Approach. IEEE Transactions on Neural Networks and Learning Systems, 2022.

  </p>
</body>
 

<br><br>







      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{jin2020pontryagin,
  title={Pontryagin differentiable programming: An end-to-end learning and control framework},
  author={Jin, Wanxin and Wang, Zhaoran and Yang, Zhuoran and Mou, Shaoshuai},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7979--7992},
  year={2020}
}
</code></pre>
    <pre><code>@article{jin2021safe,
  title={Safe pontryagin differentiable programming},
  author={Jin, Wanxin and Mou, Shaoshuai and Pappas, George J},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16034--16050},
  year={2021}
}
</code></pre>
  </div>
</section>





<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="content has-text-justified">
        <p>
         We acknowledge <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> for the website template.
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
